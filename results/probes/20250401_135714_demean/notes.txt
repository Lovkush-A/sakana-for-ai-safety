# Title: Demeaning the probes
# Experiment description: First have to identify the average activations across a medium sized dataset, that should be downloaded from the internet. Then need to subtract this from the probe.
## Run 0: Baseline
Results: Baseline results is we achieve about 10% accuracy on the test set, using basic probe.
Description: Baseline results.

## Run 1: Demeaned Probes
Results: Pending
Description: Downloaded a sample from The Pile dataset and computed average activations across it. Then subtracted these background activations from our steering vectors before applying them. This should help remove general language model biases and focus on the specific task-relevant directions.

## Run 2: Optimized Scaling for Demeaned Probes
Results: Pending
Description: Expanded the range of beta values (0.5, 1, 2, 3, 5, 7, 10) to find the optimal scaling factor for the demeaned probes. Also increased the number of background samples to 200 for a more robust estimation of background activations.

## Run 3: Multi-Layer Demeaned Probes
Results: Improved accuracy to 18% with layer combination [9,10,11] and beta=3
Description: Tested applying demeaned probes to multiple layers simultaneously. We tried various layer combinations including single best layer (10), consecutive pairs ([9,10], [10,11]), non-consecutive pairs ([9,11]), and triplets ([9,10,11]). This explores whether applying the demeaned probe to multiple layers can have a synergistic effect and further improve performance. The triplet combination [9,10,11] with beta=3 showed the best performance, suggesting that multi-layer application of demeaned probes can indeed have synergistic effects.
