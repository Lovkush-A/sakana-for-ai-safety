# Title: Demeaning the probes
# Experiment description: First have to identify the average activations across a medium sized dataset, that should be downloaded from the internet. Then need to subtract this from the probe.
## Run 0: Baseline
Results: {'layer_9.0_beta_1.0': 0.0070885481852315, 'layer_9.0_beta_3.0': 0.06547090112640801, 'layer_9.0_beta_5.0': 0.06672298706716727, 'layer_10.0_beta_1.0': 0.027939090529828934, 'layer_10.0_beta_3.0': 0.10925740508969539, 'layer_10.0_beta_5.0': 0.10842563621193158, 'layer_11.0_beta_1.0': 0.042952127659574464, 'layer_11.0_beta_3.0': 0.11717980809345009, 'layer_11.0_beta_5.0': 0.10216781393408424, 'layer_12.0_beta_1.0': 0.02252085940759277, 'layer_12.0_beta_3.0': 0.10008083020442216, 'layer_12.0_beta_5.0': 0.051295890696704205}
Description: Baseline results.

## Run 1: Basic Demeaning Implementation
Description: Used a set of 100 general language samples to compute average activations. Subtracted these general activations from the task-specific steering vectors to create demeaned probes. This should help isolate the specific signal related to the antonym task by removing general language model biases.
