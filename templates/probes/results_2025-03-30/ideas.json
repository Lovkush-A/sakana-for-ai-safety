[
    {
        "Name": "demean",
        "Title": "Demeaning the probes",
        "Experiment": "First have to identify the average activations across a large dataset. Then need to subtract this from the probe.",
        "Interestingness": 3,
        "Feasibility": 8,
        "Novelty": 3
    },
    {
        "Name": "multi_vector_steering",
        "Title": "Multi-Vector Steering for Context-Aware Antonym Generation",
        "Experiment": "1. Modify average_activations to perform k-means clustering on the activations, returning k centroids. 2. Implement a function to select the most appropriate centroid for each test prompt based on cosine similarity of the last token's embedding. 3. Modify the main loop to use the selected steering vector for each test prompt. 4. Compare results with the original single-vector method across different values of k.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6
    },
    {
        "Name": "attention_steering",
        "Title": "Attention-Focused Steering: Probing Semantic Relationships in Transformer Attention Mechanisms",
        "Experiment": "1. Modify generate_hook_addition to create hooks for specific attention heads. 2. Implement functions to apply steering vectors to query, key, and value projections in attention heads separately. 3. Extend the main loop to iterate over different attention heads and projection types (Q, K, V) instead of layers. 4. Analyze which attention heads and projection types are most effective for steering antonym generation. 5. Compare results with the original layer-wise steering method.",
        "Interestingness": 8,
        "Feasibility": 8,
        "Novelty": 7
    },
    {
        "Name": "temporal_semantic_steering",
        "Title": "Temporal Dynamics of Semantic Steering in Language Models",
        "Experiment": "1. Modify generate_text function to return intermediate tokens and their probability distributions. 2. Implement a function to calculate the probabilities of the target antonym and a set of semantically related words at each generation step. 3. Extend the main loop to generate sequences of 5-10 tokens. 4. Analyze how the probability distribution of semantically related words changes over the generated sequence, with and without steering. 5. Compare the temporal dynamics across different layers and beta values. 6. Visualize the results using heatmaps showing probability distributions over sequence length.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8
    },
    {
        "Name": "representation_drift",
        "Title": "Quantifying Representation Drift During Semantic Steering",
        "Experiment": "1. Modify generate_text to return activations at each generation step. 2. Implement a function to calculate cosine similarity between original and steered activations at each step. 3. Create a visualization function to plot this similarity over generation steps. 4. Extend the main loop to generate and analyze similarity curves for different inputs, with and without steering. 5. Compare similarity curves between successful and unsuccessful steering attempts. 6. Analyze how similarity curves differ across layers and beta values.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 8
    },
    {
        "Name": "analogy_impact",
        "Title": "Assessing Impact of Antonym Steering on Word Analogy Performance",
        "Experiment": "1. Implement a function to perform word analogy tasks using the model's output. 2. Create a small dataset of word analogy problems. 3. Modify the main loop to run the analogy task on test prompts before and after applying the steering vector. 4. Compare analogy task performance with and without steering across different layers and beta values. 5. Analyze correlations between antonym generation accuracy and changes in analogy task performance. 6. Visualize results to show how steering for antonyms affects the model's ability to solve word analogies.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7
    }
]